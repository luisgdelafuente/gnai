{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install OpenAI and Langchain to instantiate and manage LLMs, also TikToken and Chroma DB\n",
        "\n",
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install chromadb\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "QldR3UB-Quob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL of the .txt file on the web server\n",
        "url = \"https://fegalaz.usc.es/~gamallo/aulas/lingcomputacional/corpus/quijote-es.txt\"\n",
        "\n",
        "# Send a GET request to fetch the content of the .txt file\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML content\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Extract the text\n",
        "data = soup.get_text()\n",
        "\n",
        "# Print the cleaned text\n",
        "print(data)"
      ],
      "metadata": {
        "id": "21WPlm5d8HUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(data))"
      ],
      "metadata": {
        "id": "zLMPzLAXx5Fy",
        "outputId": "e4bdca61-8d2d-4fba-c713-35072659a4bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xhXwZaUrD9VJ"
      },
      "outputs": [],
      "source": [
        "# Install all the classes we need:\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.embeddings.cohere import CohereEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores.elastic_vector_search import ElasticVectorSearch\n",
        "from langchain.vectorstores import Chroma\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Activate the OpenAI API\n",
        "\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "8t1Hq6BJR-Bb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usually we will have several text files, but in this exercise we are going to split our document into several pieces or chunks and treat each of them as a separate document.\n",
        "\n",
        "The model will have to figure out which part contains the answer to our question. We break this text into multiple parts by assigning each part a maximum length using the commands below."
      ],
      "metadata": {
        "id": "2UZyaA7H5nMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This would be an exact way to cut the text in exact pieces of certain size\n",
        "# BUT I would like to have some kind of semantic criteria to get the chunks, that´s why we need text_splitter\n",
        "\n",
        "chunk_size = 250\n",
        "overlap_size = 50\n",
        "chunks = [data[i:i+chunk_size] for i in range(0, len(data) - chunk_size + 1, chunk_size - overlap_size)]\n",
        "\n",
        "len(chunks)\n"
      ],
      "metadata": {
        "id": "a1NS1rnUym_S",
        "outputId": "335e6d36-e13e-4c7f-8944-e36907be125f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10313"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UFSBDV_QD9VJ"
      },
      "outputs": [],
      "source": [
        "# !!! this needs to be reviewed\n",
        "# break the single text file into multiple parts (chunks) and treat each part as a different document.\n",
        "# Notice how langchain will try to get as close as possible to the chunk size, but not always possible.\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "texts = text_splitter.split_text(data)\n",
        "len(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we create an object that we need to save the embeddings of the various parts of the created text. BUT we want to save this texts ina permanent way so we don´t need to reconstruct them for every query with the obvious waste of resources."
      ],
      "metadata": {
        "id": "taJciStr9MNj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "EVeTcOsHD9VL"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "CMGcEq_hD9VL"
      },
      "outputs": [],
      "source": [
        "persist_directory = 'db'\n",
        "docsearch = Chroma.from_texts(\n",
        "    chunks,\n",
        "    embeddings,\n",
        "    persist_directory = persist_directory,\n",
        "    metadatas=[{\"source\": f\"{i}-pl\"} for i in range(len(chunks))]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQAWithSourcesChain"
      ],
      "metadata": {
        "id": "8TUV4hwmVbR5"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we want to turn docsearch into a retrieval because that will be its purpose.\n",
        "from langchain import OpenAI\n",
        "\n",
        "#convert the vectorstore to a retriever\n",
        "retriever=docsearch.as_retriever()"
      ],
      "metadata": {
        "id": "AU1qZhnFVbZD"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can also see the retriever what distance metric it is using; in this case default is similarity.\n",
        "retriever.search_type"
      ],
      "metadata": {
        "id": "TWQbuDo_VhbZ",
        "outputId": "882c1d99-05fe-44f6-d943-b385041ceca2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'similarity'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, we can ask the retriever to take the document that most answers one of our queries.\n",
        "# The retriever could also take more than one document if necessary.\n",
        "\n",
        "docs = retriever.get_relevant_documents(\"Escribe completa la cita de 'la razon de la sinrazón...' \")\n"
      ],
      "metadata": {
        "id": "KAO8BAKcVlEa"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And these are the docs we are going to use to build the metaprompt to query OpenAI\n",
        "\n",
        "len(docs)\n",
        "docs"
      ],
      "metadata": {
        "id": "HcQxnb8bVnDg",
        "outputId": "e7dc79f3-dcc2-4623-cd62-7c6cad3f83a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='niendo habilidad, suficiencia y entendimiento para tratar del\\nuniverso todo, pide no se desprecie su trabajo, y se le den alabanzas, no\\npor lo que escribe, sino por lo que ha dejado de escribir.\\nY luego prosigue la historia diciendo que, en acabando ', metadata={'source': '8133-pl'}),\n",
              " Document(page_content='e moche lo primero que le viene\\nal magín.\\n-Una de las tachas que ponen a la tal historia -dijo el bachiller- es que\\nsu autor puso en ella una novela intitulada El curioso impertinente; no por\\nmala ni por mal razonada, sino por no ser de aquel lugar, ', metadata={'source': '5255-pl'}),\n",
              " Document(page_content='rece apócrifa, yo no tengo la culpa; y así, sin afirmarla\\npor falsa o verdadera, la escribo. Tú, letor, pues eres prudente, juzga lo\\nque te pareciere, que yo no debo ni puedo más; puesto que se tiene por\\ncierto que al tiempo de su fin y muerte dicen ', metadata={'source': '6765-pl'}),\n",
              " Document(page_content='altar un punto a la verdad del caso. Sabed, señor, que a mí me llaman el\\nbachiller Sansón Carrasco; soy del mesmo lugar de don Quijote de la Mancha,\\ncuya locura y sandez mueve a que le tengamos lástima todos cuantos le\\nconocemos, y entre los que más ', metadata={'source': '9767-pl'})]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now what we can do is to create an agent. An agent is able to perform a series of steps to solve the user’s task on its own. Our agent will have to go and look through the documents available to it where the answer to the question asked is and return that document."
      ],
      "metadata": {
        "id": "8YnS7RS0-CYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create the chain to answer questions\n",
        "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
        "    llm = OpenAI(temperature=0.7),\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents = True\n",
        "    )"
      ],
      "metadata": {
        "id": "Lob216ynVwfy"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want, we can also create a function to post-process the agent’s output so that it is more readable."
      ],
      "metadata": {
        "id": "uSFLTAz1-_c9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_result(result):\n",
        "  print(result['answer'])\n",
        "  print(\"\\n\\n Sources : \",result['sources'] )\n",
        "  print(result['sources'])"
      ],
      "metadata": {
        "id": "Ei5Yw19qVzrA"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now everything is finally ready, we can use our agent and go and answer our queries!"
      ],
      "metadata": {
        "id": "5KoQt3N6-u9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Quien era sanson carrasco? en qué capitulos de la novela aparece ?\"\n",
        "result = chain({\"question\": question})\n",
        "process_result(result)"
      ],
      "metadata": {
        "id": "IMnzwUPUV2bH",
        "outputId": "88c8569b-3867-41e4-df15-fdea38a3f906",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Sansón Carrasco era un bachiller de 24 años, de color macilenta, de nariz chata y de boca grande. Aparece en los capítulos 5215, 5459 y 6066 de la novela.\n",
            "\n",
            "\n",
            "\n",
            " Sources :  5215-pl, 5459-pl, 6066-pl, 5216-pl\n",
            "5215-pl, 5459-pl, 6066-pl, 5216-pl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks[5285]"
      ],
      "metadata": {
        "id": "hc5Jjs3230IU",
        "outputId": "608b9297-f7c2-4e13-c9d5-9d084c363622",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sada.\\nCapítulo IV. Donde Sancho Panza satisface al bachiller Sansón Carrasco de\\nsus dudas y preguntas, con otros sucesos dignos de saberse y de contarse\\nVolvió Sancho a casa de don Quijote, y, volviendo al pasado razonamiento,\\ndijo:\\n-A lo que el seño'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}