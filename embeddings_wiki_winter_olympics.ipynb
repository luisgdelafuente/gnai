{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsimrSDXtNoa"
      },
      "source": [
        "# 1. Get Wikipedia documents\n",
        "- Retrieve the pages related to a specific wikipedia category."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mwclient\n",
        "!pip install mwparserfromhell\n",
        "!pip install openai\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "2P4Kqj-RdUJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhGatjcmtNoh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecefad3b-fada-40ae-d917-11a60a4d4117"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# Creating datasets from wikipedia\n",
        "\n",
        "# imports\n",
        "import mwclient             # for downloading example Wikipedia articles\n",
        "import mwparserfromhell     # for splitting Wikipedia articles into sections\n",
        "import openai               # for generating embeddings\n",
        "import pandas as pd         # for DataFrames to store article sections and embeddings\n",
        "import re                   # for cutting <ref> links out of Wikipedia articles\n",
        "import tiktoken             # for counting tokens\n",
        "\n",
        "openai.api_key = \"sk-\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUHu64fBtNoi",
        "outputId": "7d371fb7-cc25-4c09-e2fa-60ae2a9dd792",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 732 article titles in Category:2022 Winter Olympics.\n"
          ]
        }
      ],
      "source": [
        "# get Wikipedia pages from a category\n",
        "\n",
        "CATEGORY_TITLE = \"Category:2022 Winter Olympics\"\n",
        "WIKI_SITE = \"en.wikipedia.org\"\n",
        "\n",
        "def titles_from_category(\n",
        "    category: mwclient.listing.Category, max_depth: int\n",
        "    ) -> set[str]:\n",
        "    \"\"\"Return a set of page titles in a given Wiki category and its subcategories.\"\"\"\n",
        "    titles = set()\n",
        "    for cm in category.members():\n",
        "        if type(cm) == mwclient.page.Page:\n",
        "            # ^type() used instead of isinstance() to catch match w/ no inheritance\n",
        "            titles.add(cm.name)\n",
        "        elif isinstance(cm, mwclient.listing.Category) and max_depth > 0:\n",
        "            deeper_titles = titles_from_category(cm, max_depth=max_depth - 1)\n",
        "            titles.update(deeper_titles)\n",
        "    return titles\n",
        "\n",
        "\n",
        "site = mwclient.Site(WIKI_SITE)\n",
        "category_page = site.pages[CATEGORY_TITLE]\n",
        "titles = titles_from_category(category_page, max_depth=1)\n",
        "\n",
        "# ^note: max_depth=1 means we go one level deep in the category tree\n",
        "print(f\"Found {len(titles)} article titles in {CATEGORY_TITLE}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omod0tBdtNok"
      },
      "source": [
        "# 2.Chunk documents\n",
        "\n",
        "Now that we have our reference documents, we need to prepare them for search.Because GPT can only read a limited amount of text at once, we'll split each document into chunks short enough to be read.\n",
        "\n",
        "For this specific example on Wikipedia articles, we'll:\n",
        "\n",
        "- Discard less relevant-looking sections like External Links and Footnotes\n",
        "- Clean up the text by removing reference tags (e.g., ), whitespace, and super short sections\n",
        "- Split each article into sections\n",
        "- Prepend titles and subtitles to each section's text, to help GPT understand the context\n",
        "- If a section is long (say, > 1,600 tokens), we'll recursively split it into smaller sections, trying to - split along semantic boundaries like paragraphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7ldHXuktNok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb6f88fb-e7db-4046-d764-c2548c669f4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# define functions to split Wikipedia pages into sections\n",
        "\n",
        "SECTIONS_TO_IGNORE = [\n",
        "    \"See also\",\n",
        "    \"References\",\n",
        "    \"External links\",\n",
        "    \"Further reading\",\n",
        "    \"Footnotes\",\n",
        "    \"Bibliography\",\n",
        "    \"Sources\",\n",
        "    \"Citations\",\n",
        "    \"Literature\",\n",
        "    \"Footnotes\",\n",
        "    \"Notes and references\",\n",
        "    \"Photo gallery\",\n",
        "    \"Works cited\",\n",
        "    \"Photos\",\n",
        "    \"Gallery\",\n",
        "    \"Notes\",\n",
        "    \"References and sources\",\n",
        "    \"References and notes\",\n",
        "]\n",
        "\n",
        "\n",
        "def all_subsections_from_section(\n",
        "    section: mwparserfromhell.wikicode.Wikicode,\n",
        "    parent_titles: list[str],\n",
        "    sections_to_ignore: set[str],\n",
        ") -> list[tuple[list[str], str]]:\n",
        "    \"\"\"\n",
        "    From a Wikipedia section, return a flattened list of all nested subsections.\n",
        "    Each subsection is a tuple, where:\n",
        "        - the first element is a list of parent subtitles, starting with the page title\n",
        "        - the second element is the text of the subsection (but not any children)\n",
        "    \"\"\"\n",
        "    headings = [str(h) for h in section.filter_headings()]\n",
        "    title = headings[0]\n",
        "    if title.strip(\"=\" + \" \") in sections_to_ignore:\n",
        "        # ^wiki headings are wrapped like \"== Heading ==\"\n",
        "        return []\n",
        "    titles = parent_titles + [title]\n",
        "    full_text = str(section)\n",
        "    section_text = full_text.split(title)[1]\n",
        "    if len(headings) == 1:\n",
        "        return [(titles, section_text)]\n",
        "    else:\n",
        "        first_subtitle = headings[1]\n",
        "        section_text = section_text.split(first_subtitle)[0]\n",
        "        results = [(titles, section_text)]\n",
        "        for subsection in section.get_sections(levels=[len(titles) + 1]):\n",
        "            results.extend(all_subsections_from_section(subsection, titles, sections_to_ignore))\n",
        "        return results\n",
        "\n",
        "\n",
        "def all_subsections_from_title(\n",
        "    title: str,\n",
        "    sections_to_ignore: set[str] = SECTIONS_TO_IGNORE,\n",
        "    site_name: str = WIKI_SITE,\n",
        ") -> list[tuple[list[str], str]]:\n",
        "    \"\"\"From a Wikipedia page title, return a flattened list of all nested subsections.\n",
        "    Each subsection is a tuple, where:\n",
        "        - the first element is a list of parent subtitles, starting with the page title\n",
        "        - the second element is the text of the subsection (but not any children)\n",
        "    \"\"\"\n",
        "    site = mwclient.Site(site_name)\n",
        "    page = site.pages[title]\n",
        "    text = page.text()\n",
        "    parsed_text = mwparserfromhell.parse(text)\n",
        "    headings = [str(h) for h in parsed_text.filter_headings()]\n",
        "    if headings:\n",
        "        summary_text = str(parsed_text).split(headings[0])[0]\n",
        "    else:\n",
        "        summary_text = str(parsed_text)\n",
        "    results = [([title], summary_text)]\n",
        "    for subsection in parsed_text.get_sections(levels=[2]):\n",
        "        results.extend(all_subsections_from_section(subsection, [title], sections_to_ignore))\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsbVxm8TtNom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44e0a3a8-5611-4264-f974-7474aab9c619"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5730 sections in 732 pages.\n"
          ]
        }
      ],
      "source": [
        "# split pages into sections\n",
        "# may take ~1 minute per 100 articles\n",
        "\n",
        "wikipedia_sections = []\n",
        "for title in titles:\n",
        "    wikipedia_sections.extend(all_subsections_from_title(title))\n",
        "print(f\"Found {len(wikipedia_sections)} sections in {len(titles)} pages.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8TGyalEtNoq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f57dd2f-39b8-498d-b1b7-865ad8a89036"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered out 530 sections, leaving 5200 sections.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# clean text\n",
        "def clean_section(section: tuple[list[str], str]) -> tuple[list[str], str]:\n",
        "    \"\"\"\n",
        "    Return a cleaned up section with:\n",
        "        - <ref>xyz</ref> patterns removed\n",
        "        - leading/trailing whitespace removed\n",
        "    \"\"\"\n",
        "    titles, text = section\n",
        "    text = re.sub(r\"<ref.*?</ref>\", \"\", text)\n",
        "    text = text.strip()\n",
        "    return (titles, text)\n",
        "\n",
        "\n",
        "wikipedia_sections = [clean_section(ws) for ws in wikipedia_sections]\n",
        "\n",
        "# filter out short/blank sections\n",
        "def keep_section(section: tuple[list[str], str]) -> bool:\n",
        "    \"\"\"Return True if the section should be kept, False otherwise.\"\"\"\n",
        "    titles, text = section\n",
        "    if len(text) < 16:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "\n",
        "original_num_sections = len(wikipedia_sections)\n",
        "wikipedia_sections = [ws for ws in wikipedia_sections if keep_section(ws)]\n",
        "print(f\"Filtered out {original_num_sections - len(wikipedia_sections)} sections, leaving {len(wikipedia_sections)} sections.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ivi5hQr0tNoq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "outputId": "f210e54f-621d-4b99-8b53-809ed585d885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Petteri Lindbohm']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'{{short description|Finnish ice hockey player}}\\n{{Use mdy dates|date=April 20...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "['Petteri Lindbohm', '==Playing career==']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Lindbohm played in his native Finnish [[Liiga]] during the [[2012–13 SM-liiga...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "['Petteri Lindbohm', '==International play==']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'{{MedalTableTop|name=}}\\n{{MedalSport| [[Ice hockey]]}}\\n{{MedalCountry|{{ih|FI...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "['Petteri Lindbohm', '==Career statistics==', '=== Regular season and playoffs ===']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'{| border=\"0\" cellpadding=\"1\" cellspacing=\"0\" style=\"text-align:center; width...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "['Petteri Lindbohm', '==Career statistics==', '===International===']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'{| border=\"0\" cellpadding=\"1\" cellspacing=\"0\" ID=\"Table3\" style=\"text-align:c...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "['Zoi Sadowski-Synnott']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'{{short description|New Zealand snowboarder (born 2001)}}\\n{{Use dmy dates|dat...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "['Zoi Sadowski-Synnott', '==Early life and family==']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Sadowski-Synnott was born in [[Sydney]], [[New South Wales]], Australia, to a...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "['Zoi Sadowski-Synnott', '==Career==']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Sadowski-Synnott won the silver medal in the slopestyle at the [[FIS Freestyl...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "['Zoi Sadowski-Synnott', '==International competitions==']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'{| {{AchievementTable|Event=yes}}\\n|-\\n|2017\\n|[[FIS Freestyle Ski and Snowboard...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "['Cam Stones']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'{{short description|Canadian bobsledder}}\\n{{Use dmy dates|date=January 2020}}...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "['Lee Yu-bin']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'{{short description|South Korean short track speedskater}}\\n{{family name hatn...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "['Candy Bauer']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'{{short description|German bobsledder}}\\n{{Use dmy dates|date=July 2020}}\\n{{In...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# print example data\n",
        "for ws in wikipedia_sections[:12]:\n",
        "    print(ws[0])\n",
        "    display(ws[1][:77] + \"...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxWWUmW8tNor"
      },
      "source": [
        "Next, we'll recursively split long sections into smaller sections.\n",
        "There's no perfect recipe for splitting text into sections.\n",
        "Some tradeoffs include:\n",
        "- Longer sections may be better for questions that require more context\n",
        "- Longer sections may be worse for retrieval, as they may have more topics muddled together\n",
        "- Shorter sections are better for reducing costs (which are proportional to the number of tokens)\n",
        "- Shorter sections allow more sections to be retrieved, which may help with recall\n",
        "- Overlapping sections may help prevent answers from being cut by section boundaries\n",
        "- Here, we'll use a simple approach and limit sections to 1,600 tokens each, recursively halving any sections that are too long. To avoid cutting in the middle of useful sentences, we'll split along paragraph boundaries when possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0I-JHAgltNor",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acc13b9c-7e06-4ea2-917f-c985d39c822f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "GPT_MODEL = \"gpt-3.5-turbo\"  # only matters insofar as it selects which tokenizer to use\n",
        "\n",
        "\n",
        "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
        "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "\n",
        "def halved_by_delimiter(string: str, delimiter: str = \"\\n\") -> list[str, str]:\n",
        "    \"\"\"Split a string in two, on a delimiter, trying to balance tokens on each side.\"\"\"\n",
        "    chunks = string.split(delimiter)\n",
        "    if len(chunks) == 1:\n",
        "        return [string, \"\"]  # no delimiter found\n",
        "    elif len(chunks) == 2:\n",
        "        return chunks  # no need to search for halfway point\n",
        "    else:\n",
        "        total_tokens = num_tokens(string)\n",
        "        halfway = total_tokens // 2\n",
        "        best_diff = halfway\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            left = delimiter.join(chunks[: i + 1])\n",
        "            left_tokens = num_tokens(left)\n",
        "            diff = abs(halfway - left_tokens)\n",
        "            if diff >= best_diff:\n",
        "                break\n",
        "            else:\n",
        "                best_diff = diff\n",
        "        left = delimiter.join(chunks[:i])\n",
        "        right = delimiter.join(chunks[i:])\n",
        "        return [left, right]\n",
        "\n",
        "\n",
        "def truncated_string(\n",
        "    string: str,\n",
        "    model: str,\n",
        "    max_tokens: int,\n",
        "    print_warning: bool = True,\n",
        ") -> str:\n",
        "    \"\"\"Truncate a string to a maximum number of tokens.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    encoded_string = encoding.encode(string)\n",
        "    truncated_string = encoding.decode(encoded_string[:max_tokens])\n",
        "    if print_warning and len(encoded_string) > max_tokens:\n",
        "        print(f\"Warning: Truncated string from {len(encoded_string)} tokens to {max_tokens} tokens.\")\n",
        "    return truncated_string\n",
        "\n",
        "\n",
        "def split_strings_from_subsection(\n",
        "    subsection: tuple[list[str], str],\n",
        "    max_tokens: int = 1000,\n",
        "    model: str = GPT_MODEL,\n",
        "    max_recursion: int = 5,\n",
        ") -> list[str]:\n",
        "    \"\"\"\n",
        "    Split a subsection into a list of subsections, each with no more than max_tokens.\n",
        "    Each subsection is a tuple of parent titles [H1, H2, ...] and text (str).\n",
        "    \"\"\"\n",
        "    titles, text = subsection\n",
        "    string = \"\\n\\n\".join(titles + [text])\n",
        "    num_tokens_in_string = num_tokens(string)\n",
        "    # if length is fine, return string\n",
        "    if num_tokens_in_string <= max_tokens:\n",
        "        return [string]\n",
        "    # if recursion hasn't found a split after X iterations, just truncate\n",
        "    elif max_recursion == 0:\n",
        "        return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
        "    # otherwise, split in half and recurse\n",
        "    else:\n",
        "        titles, text = subsection\n",
        "        for delimiter in [\"\\n\\n\", \"\\n\", \". \"]:\n",
        "            left, right = halved_by_delimiter(text, delimiter=delimiter)\n",
        "            if left == \"\" or right == \"\":\n",
        "                # if either half is empty, retry with a more fine-grained delimiter\n",
        "                continue\n",
        "            else:\n",
        "                # recurse on each half\n",
        "                results = []\n",
        "                for half in [left, right]:\n",
        "                    half_subsection = (titles, half)\n",
        "                    half_strings = split_strings_from_subsection(\n",
        "                        half_subsection,\n",
        "                        max_tokens=max_tokens,\n",
        "                        model=model,\n",
        "                        max_recursion=max_recursion - 1,\n",
        "                    )\n",
        "                    results.extend(half_strings)\n",
        "                return results\n",
        "    # otherwise no split was found, so just truncate (should be very rare)\n",
        "    return [truncated_string(string, model=model, max_tokens=max_tokens)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03gceD3EtNor",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a26ee04f-272e-41f8-e0ee-b2dd88d707ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5200 Wikipedia sections split into 6059 strings.\n"
          ]
        }
      ],
      "source": [
        "# split sections into chunks\n",
        "MAX_TOKENS = 1600\n",
        "wikipedia_strings = []\n",
        "for section in wikipedia_sections:\n",
        "    wikipedia_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))\n",
        "\n",
        "print(f\"{len(wikipedia_sections)} Wikipedia sections split into {len(wikipedia_strings)} strings.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWHyf5gatNos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c9dfa1c-c714-4579-c3be-7d79a509313a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Petteri Lindbohm\n",
            "\n",
            "==Playing career==\n",
            "\n",
            "Lindbohm played in his native Finnish [[Liiga]] during the [[2012–13 SM-liiga season|2012–13]] and [[2013–14 Liiga season|2013–14]] seasons. On March 21, 2014, he signed a three-year, entry-level deal with the [[St. Louis Blues]].\n",
            "\n",
            "In the [[2014-15 NHL season|2014–15]] season, his first in North America, Lindbohm primarily played with the Blues' [[American Hockey League|AHL]] affiliate [[Chicago Wolves]], appearing in 53 games. Lindbohm also skated in 23 games for the Blues, scoring two goals and 3 points. Lindbohm scored his first career NHL goal on February 20, 2015, in a 5-1 victory over the [[Boston Bruins]].\n",
            "\n",
            "On September 11, 2017, the Blues re-signed Lindbohm as a restricted free agent to a one-year, two-way contract. He was re-assigned to continue his tenure with the Chicago Wolves in the [[2017-18 AHL season|2017–18]] season.\n",
            "\n",
            "On July 27, 2018, as a restricted free agent from the Blues, Lindbohm left North America and signed a one-year contract in Switzerland worth CHF 700,000 with [[Lausanne HC]] of the [[National League (ice hockey)|National League]] (NL). Right after winning gold with Finland at the [[2019 IIHF World Championship]], Lausanne signed Lindbohm to a one-year contract extension worth CHF 750,000. On May 20, 2020, Lausanne HC announced that Lindbohm would not return to the team for the 2020–21 season.\n",
            "\n",
            "On July 30, 2020, Lindbohm agreed to a one-year contract with [[EHC Biel]] to remain in the National League for the [[2020-21 NL season|2020–21]] season.\n",
            "\n",
            "Following his third season in the NL, Lindbohm left as a free agent and returned to his original club, Jokerit, now of the KHL, on a two-year agreement on May 5, 2021. In the [[2021-22 KHL season|2021–22]] season, Lindbohm playing in a top-four shutdown role registered three goals and 8 points from the blueline, helping Jokerit to a second place finish in the regular season.\n",
            "\n",
            "With Jokerit withdrawing from the KHL playoffs, Lindbohm was released from his contract as a free agent and returned to the NHL in agreeing to a one-year, $750,000 contract with the Florida Panthers for the remainder of the {{nhly|2021}} season on March 1, 2022.\n",
            "\n",
            "As a free agent from the Panthers, Lindbohm returned to Europe and agreed on a one-year contract to play with his first Swedish club, Frölunda HC of the SHL, on August 2, 2022.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# print example data\n",
        "print(wikipedia_strings[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUGbQ-8ItNos"
      },
      "source": [
        "# 3. Embed document chunks\n",
        "Now that we've split our library into shorter self-contained strings, we can compute embeddings for each.\n",
        "\n",
        "(For large embedding jobs, use a script like api_request_parallel_processor.py to parallelize requests while throttling to stay under rate limits.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwMQvRIftNos",
        "outputId": "e2f31b35-7d8f-4198-ba67-17a83223142d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 to 999\n",
            "Batch 1000 to 1999\n",
            "Batch 2000 to 2999\n",
            "Batch 3000 to 3999\n",
            "Batch 4000 to 4999\n",
            "Batch 5000 to 5999\n",
            "Batch 6000 to 6999\n"
          ]
        }
      ],
      "source": [
        "# calculate embeddings\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"  # OpenAI's best embeddings as of Apr 2023\n",
        "BATCH_SIZE = 1000  # you can submit up to 2048 embedding inputs per request\n",
        "\n",
        "embeddings = []\n",
        "for batch_start in range(0, len(wikipedia_strings), BATCH_SIZE):\n",
        "    batch_end = batch_start + BATCH_SIZE\n",
        "    batch = wikipedia_strings[batch_start:batch_end]\n",
        "    print(f\"Batch {batch_start} to {batch_end-1}\")\n",
        "    response = openai.Embedding.create(model=EMBEDDING_MODEL, input=batch)\n",
        "    for i, be in enumerate(response[\"data\"]):\n",
        "        assert i == be[\"index\"]  # double check embeddings are in same order as input\n",
        "    batch_embeddings = [e[\"embedding\"] for e in response[\"data\"]]\n",
        "    embeddings.extend(batch_embeddings)\n",
        "\n",
        "df = pd.DataFrame({\"text\": wikipedia_strings, \"embedding\": embeddings})\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9uZEPcftNot"
      },
      "source": [
        "# 4. Store document chunks and embeddings\n",
        "Because this example only uses a few thousand strings, we'll store them in a CSV file.\n",
        "For larger datasets, use a vector database, which will be more performant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IdnWTIXtNot"
      },
      "outputs": [],
      "source": [
        "# save document chunks and embeddings\n",
        "\n",
        "SAVE_PATH = \"C:/Users/luisg/Downloads/eleccion.csv\"\n",
        "\n",
        "df.to_csv(SAVE_PATH, index=False)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}