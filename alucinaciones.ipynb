{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOTvzGU2L9W8RwnGD/NvH/0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luisgdelafuente/gnai/blob/main/alucinaciones.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alucinaciones\n",
        "\n",
        "Vamos a someter al modelo a una serie de preguntas relacionadas con las olimpiadas de 2020, con la que no ha sido entrenado. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9iD2LZ8S_Rsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar las librerías que necesitamos\n",
        "\n",
        "!pip install openai\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "O9xaOcfbRoYo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edcc2fc0-7c04-47e9-862f-0c18557908c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjHrd5EhRNLD"
      },
      "outputs": [],
      "source": [
        "# Importar métodos, API y crear una variable ambiente con el modelo que vamos a usar\n",
        "\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from transformers import GPT2TokenizerFast\n",
        "from typing import Dict, Tuple, List\n",
        "\n",
        "openai.api_key = \"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\"\n",
        "COMPLETIONS_MODEL = \"text-davinci-003\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora hacemos una consulta sobre la que el modelo no haya sido entrenado para que el modelo alucine y se invente la respuesta. \n",
        "\n",
        "(Marcelo Chierghini es un olimpista brasileño, pero es nadador y quedó en el puesto 8 de las olimpiadas).\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "tw-hRjAuP_zW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Who won the 2020 Summer Olympics men's high jump?\"\n",
        "# prompt = \"Quién ganó las olimpiadas de verano 2020 en la categoría de salto de altura de hombres?\"\n",
        "\n",
        "openai.Completion.create(\n",
        "    prompt=prompt,\n",
        "    temperature=0,\n",
        "    max_tokens=300,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0,\n",
        "    model=COMPLETIONS_MODEL\n",
        ")[\"choices\"][0][\"text\"].strip(\" \\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nPMxyKUOSMKQ",
        "outputId": "b65def72-10e0-485f-fdf2-d5f5109ebfe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Marcelo Chierighini of Brazil won the gold medal in the men's high jump at the 2020 Summer Olympics.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prevenir alucinaciones\n",
        "\n",
        "Vamos a tratar una serie de métodos para prevenir las alucinaciones por este orden: \n",
        "- Hacer reconocer al modelo que no sabe la respuesta (en este notebook)\n",
        "- Mejorar la consulta, es decir el prompt (en este notebook)\n",
        "- Utilizar embeddings.\n",
        "- Fine-tunear el modelo. "
      ],
      "metadata": {
        "id": "ggF7p2OoNCiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hacer que el modelo responda que no sabe a respuesta: \n",
        "\n",
        "prompt = \"\"\"Responde a la pregunta lo más verídicamente posible, y si no estás seguro de la respuesta responde 'Lo siento, no lo sé'. \n",
        "\n",
        "Q: Quién ganó las olimpiadas de verano 2020 en la categoría de salto de altura de hombres? \n",
        "A:\"\"\"\n",
        "\n",
        "openai.Completion.create(\n",
        "    prompt=prompt,\n",
        "    temperature=0,\n",
        "    max_tokens=300,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0,\n",
        "    model=COMPLETIONS_MODEL\n",
        ")[\"choices\"][0][\"text\"].strip(\" \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "iicfJAurTe3_",
        "outputId": "ed9c73e5-d12f-4b62-f219-9db219b566b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Lo siento, no lo sé.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mejorar el prompt dando pistas sobre la respuesta, además de lo anterior. \n",
        "# Observar cómo si la cantidad de contexto no es muy grande, podemos incluirla en el prompt directamente. \n",
        "\n",
        "prompt = \"\"\"Answer the question as truthfully as possible using the provided text, and if the answer is not contained within the text below, say \"I don't know\"\n",
        "\n",
        "Context:\n",
        "The men's high jump event at the 2020 Summer Olympics took place between 30 July and 1 August 2021 at the Olympic Stadium.\n",
        "33 athletes from 24 nations competed; the total possible number depended on how many nations would use universality places \n",
        "to enter athletes in addition to the 32 qualifying through mark or ranking (no universality places were used in 2021).\n",
        "Italian athlete Gianmarco Tamberi along with Qatari athlete Mutaz Essa Barshim emerged as joint winners of the event following\n",
        "a tie between both of them as they cleared 2.37m. Both Tamberi and Barshim agreed to share the gold medal in a rare instance\n",
        "where the athletes of different nations had agreed to share the same medal in the history of Olympics. \n",
        "Barshim in particular was heard to ask a competition official \"Can we have two golds?\" in response to being offered a \n",
        "'jump off'. Maksim Nedasekau of Belarus took bronze. The medals were the first ever in the men's high jump for Italy and \n",
        "Belarus, the first gold in the men's high jump for Italy and Qatar, and the third consecutive medal in the men's high jump\n",
        "for Qatar (all by Barshim). Barshim became only the second man to earn three medals in high jump, joining Patrik Sjöberg\n",
        "of Sweden (1984 to 1992).\n",
        "\n",
        "Q: Who won the 2020 Summer Olympics men's high jump?\n",
        "A:\"\"\"\n",
        "\n",
        "openai.Completion.create(\n",
        "    prompt=prompt,\n",
        "    temperature=0,\n",
        "    max_tokens=300,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0,\n",
        "    model=COMPLETIONS_MODEL\n",
        ")[\"choices\"][0][\"text\"].strip(\" \\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "K68viha-T-18",
        "outputId": "ef3e3fd0-62e2-4823-8225-8afb0ee1b1ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Gianmarco Tamberi and Mutaz Essa Barshim emerged as joint winners of the event.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Haciendo algo de prompt engineering hemos conseguido que el modelo de una respuesta acertada, pero ¿a qué precio? este sistema no es práctico porque no escala: no podemos enviar la totalidad del contexto cada vez que hacemos consultas, y mucho menos el dataset completo cuando éstas son imprevisibles. \n",
        "\n",
        "Aquí es donde tienen sentido los Embeddings. \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9J6PtaQqOdVh"
      }
    }
  ]
}