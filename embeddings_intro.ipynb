{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilzar Embedings\n",
    "\n",
    "¿Qué hacemos cuando tenemos un dataser enorme de información que nes necesario parsear? Aquí es donde tienen sentido la Embedding API. \n",
    "\n",
    "Vamos a ver cómo proporcional información contextual para que la API nos de las respuestas correctas. siguiendo estos pasos: \n",
    "- Primero extrae la información relevante de la pregunta\n",
    "- Luego utiliza la Completions API para responder correctamente. \n",
    "\n",
    "De forma más específica los pasos a seguir son: \n",
    "- Pre-procesar la información contextual troceándola en piezas más pequeñas y creando un vector embebido para cada una. \n",
    "- Cuando se recibe una consulta, embeberla consulta en el mismo espacio vectorial que los trozos, y encontrando el contexto más relevante a dicha consulta. \n",
    "- Añadir el contexto relevante al prompt de la consulta (como hemos hecho más arriba). \n",
    "- Enviar a GPT3 el prmpt y recibir una respuesta que si tiene en cuenta la información contextual. \n",
    "\n",
    "\n",
    "## Paso 1: procesar la información contextual\n",
    "\n",
    "Vamos a procesar un documento con el corpus de datos y a trocearlo de forma que cada sección tenga suficiente información como para responder a una pregunta, pero sea lo suficientemente pequeño como para poder adjuntarse a uno o varios prompts. \n",
    "\n",
    "En general una frase o un párrafo corto funcionan, PERO puede ser necesario adaptar el tamaño a cada dataset. \n",
    "\n",
    "Ahora el documento queda dividido en secciones, y cada una de estas secciones ha quedado vectorizada. \n",
    "\n",
    "!!! Notice there´s lots of friction in the process of indexing this information: each client has tons of unstructured and duplicated data. \n",
    "\n",
    "\n",
    "¡El siguiente paso es usar estos embedings para responder a las preguntas del usuario!\n",
    "\n",
    "## Paso 2: Encontrar documentos embebidos relevantes para la query\n",
    "\n",
    "Para poder responder a la pregunta del usuario lo primero que hacemos es vectorizar la pregunta y recuperar los vectores relevantes del documento. \n",
    "\n",
    "En este caso concreto los vectores están almacenados localmente, pero para modelos más grandes debemos usar un buscador de vectores (search engine vector) como Pinecone o Weaviate. \n",
    "- Definamos primero la similitud entre vectores\n",
    "- Ahora queremos comparar el vector de la query con los del documento, para saber los más relevantes de éste último: \n",
    "\n",
    "\n",
    "# Paso 3. Construir el metaprompt añadiendo vectores relevantes al de la query \n",
    "- As the OpenAI notebook suggests: it's \"helpful to use a query separator to help the model distinguish between separate pieces of text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
