{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training GPT-3 on a custom use case dataset \n",
    "\n",
    "This allows the model to better adapt to the nuance of that specific use case or domain, leading to more accurate results. This specific sample is not working. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dictionaries with a very small dataset of QAs\n",
    "\n",
    "training_data = [\n",
    "\t{\n",
    "    \t\"prompt\": \"Cual es la capital de España (dime algo incorrecto)?->\",\n",
    "    \t\"completion\": \"\"\" La capital de España es Cercedilla.\\n\"\"\"\n",
    "\t},\n",
    "\t{\n",
    "    \t\"prompt\": \"What is the primary function of the heart?->\",\n",
    "    \t\"completion\": \"\"\" The primary function of the heart is to pump blood throughout the body.\\n\"\"\"\n",
    "\t},\n",
    "\t{\n",
    "    \t\"prompt\": \"What is photosynthesis?->\",\n",
    "    \t\"completion\": \"\"\" Photosynthesis is the process by which green plants and some other organisms convert sunlight into chemical energy stored in the form of glucose.\\n\"\"\"\n",
    "\t},\n",
    "\t{\n",
    "    \t\"prompt\": \"Who wrote the play 'Romeo and Juliet'?->\",\n",
    "    \t\"completion\": \"\"\" William Shakespeare wrote the play 'Romeo and Juliet'.\\n\"\"\"\n",
    "\t},\n",
    "\t{\n",
    "    \t\"prompt\": \"Which element has the atomic number 1?->\",\n",
    "    \t\"completion\": \"\"\" Hydrogen has the atomic number 1.\\n\"\"\"\n",
    "\t},\n",
    "\t{\n",
    "    \t\"prompt\": \"What is the largest planet in our solar system?->\",\n",
    "    \t\"completion\": \"\"\" Jupiter is the largest planet in our solar system.\\n\"\"\"\n",
    "\t},\n",
    "\t{\n",
    "    \t\"prompt\": \"What is the freezing point of water in Celsius?->\",\n",
    "    \t\"completion\": \"\"\" The freezing point of water in Celsius is 0 degrees.\\n\"\"\"\n",
    "\t},\n",
    "\t{\n",
    "    \t\"prompt\": \"What is the square root of 144?->\",\n",
    "    \t\"completion\": \"\"\" The square root of 144 is 12.\\n\"\"\"\n",
    "\t},\n",
    "\t{\n",
    "    \t\"prompt\": \"Who is the author of 'To Kill a Mockingbird'?->\",\n",
    "    \t\"completion\": \"\"\" The author of 'To Kill a Mockingbird' is Harper Lee.\\n\"\"\"\n",
    "\t},\n",
    "\t{\n",
    "    \t\"prompt\": \"What is the smallest unit of life?->\",\n",
    "    \t\"completion\": \"\"\" The smallest unit of life is the cell.\\n\"\"\"\n",
    "\t}\n",
    "]\n",
    "\n",
    "validation_data = [\n",
    "\t{\n",
    "    \t\"prompt\": \"Which gas do plants use for photosynthesis?->\",\n",
    "    \t\"completion\": \"\"\" Plants use carbon dioxide for photosynthesis.\\n\"\"\"\n",
    "\t},\n",
    "\t{\n",
    "    \t\"prompt\": \"What are the three primary colors of light?->\",\n",
    "    \t\"completion\": \"\"\" The three primary colors of light are red, green, and blue.\\n\"\"\"\n",
    "\t},\n",
    "\t{\n",
    "    \t\"prompt\": \"Who discovered penicillin?->\",\n",
    "    \t\"completion\": \"\"\" Sir Alexander Fleming discovered penicillin.\\n\"\"\"\n",
    "\t},\n",
    "\t{\n",
    "    \t\"prompt\": \"What is the chemical formula for water?->\",\n",
    "    \t\"completion\": \"\"\" The chemical formula for water is H2O.\\n\"\"\"\n",
    "\t},\n",
    "\t{\n",
    "    \t\"prompt\": \"What is the largest country by land area?->\",\n",
    "    \t\"completion\": \"\"\" Russia is the largest country by land area.\\n\"\"\"\n",
    "\t},\n",
    "\t{\n",
    "    \t\"prompt\": \"What is the speed of light in a vacuum?->\",\n",
    "    \t\"completion\": \"\"\" The speed of light in a vacuum is approximately 299,792 kilometers per second.\\n\"\"\"\n",
    "\t},\n",
    "\t{\n",
    "    \t\"prompt\": \"What is the currency of Japan?->\",\n",
    "    \t\"completion\": \"\"\" The currency of Japan is the Japanese Yen.\\n\"\"\"\n",
    "\t},\n",
    "\t{\n",
    "    \t\"prompt\": \"What is the smallest bone in the human body?->\",\n",
    "    \t\"completion\": \"\"\" The stapes, located in the middle ear, is the smallest bone in the human body.\\n\"\"\"\n",
    "\t}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code leverages the helper function prepare_data to create both the training and validation data in JSONL formats:\n",
    "\n",
    "import json\n",
    "\n",
    "def prepare_data(dictionary_data, final_file_name):\n",
    "    with open(final_file_name, 'w') as outfile:\n",
    "        for entry in dictionary_data:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "# Call the prepare_data function for training and validation data\n",
    "prepare_data(training_data, \"training_data.jsonl\")\n",
    "prepare_data(validation_data, \"validation_data.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Your file contains 10 prompt-completion pairs. In general, we recommend having at least a few hundred examples. We've found that performance tends to linearly increase for every doubling of the number of examples\n",
      "- All prompts end with suffix `?->`\n",
      "- All completions end with suffix `.\\n`\n",
      "\n",
      "No remediations found.\n",
      "\n",
      "You can use your file for fine-tuning:\n",
      "> openai api fine_tunes.create -t \"training_data.jsonl\"\n",
      "\n",
      "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string `?->` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\".\\n\"]` so that the generated texts ends at the expected place.\n",
      "Once your model starts training, it'll approximately take 2.58 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n",
      "Analyzing...\n",
      "\n",
      "- Your file contains 8 prompt-completion pairs. In general, we recommend having at least a few hundred examples. We've found that performance tends to linearly increase for every doubling of the number of examples\n",
      "- All prompts end with suffix `?->`\n",
      "- All prompts start with prefix `Wh`\n",
      "- All completions end with suffix `.\\n`\n",
      "\n",
      "No remediations found.\n",
      "\n",
      "You can use your file for fine-tuning:\n",
      "> openai api fine_tunes.create -t \"validation_data.jsonl\"\n",
      "\n",
      "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string `?->` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\".\\n\"]` so that the generated texts ends at the expected place.\n",
      "Once your model starts training, it'll approximately take 2.56 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
     ]
    }
   ],
   "source": [
    "# The preparation of the datasets can be finalized using the following statements for both the training and the validation data.\n",
    "# These commands are likely used to prepare training and validation data in JSONL format for fine-tuning an OpenAI GPT model using OpenAI's command-line tools. \n",
    "\n",
    "!openai tools fine_tunes.prepare_data -f \"training_data.jsonl\"\n",
    "!openai tools fine_tunes.prepare_data -f \"validation_data.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training File ID: file-2sCcE5tGNelmNGrMElshJlMw\n",
      "Validation File ID: file-YdohajIDRBOTOZn0wRphiJ8z\n"
     ]
    }
   ],
   "source": [
    "# Finally, we upload the two datasets to the OpenAI developer account as follows:\n",
    "\n",
    "import openai\n",
    "\n",
    "# Define the file names and purposes\n",
    "training_file_name = \"training_data.jsonl\"\n",
    "validation_file_name = \"validation_data.jsonl\"\n",
    "\n",
    "# Upload the training dataset\n",
    "training_file = openai.File.create(file=open(training_file_name, 'rb'), purpose='fine-tune')\n",
    "\n",
    "# Upload the validation dataset\n",
    "validation_file = openai.File.create(file=open(validation_file_name, 'rb'), purpose='fine-tune')\n",
    "\n",
    "print(f\"Training File ID: {training_file.id}\")\n",
    "print(f\"Validation File ID: {validation_file.id}\")\n",
    "\n",
    "# Successful execution of the previous code displays below the unique identifier of the training and validation data.\n",
    "# so far we have collected, formatted and uploaded the data. Now we are ready for the fine-tune!. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a fine-tuning job\n",
    "\n",
    "- This fine-tuning process is highly inspired by the openai-cookbook performing fine-tuning on Microsoft Azure.\n",
    "- To perform the fine-tuning we will use the following two steps: (1) define hyperparameters, and (2) trigger the fine-tuning.\n",
    "- We will fine-tune the davinci model and run it for 15 epochs using a batch size of 3 and a learning rate multiplier of 0.3 using the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tunning model with jobID: ft-tj3gzvmjl4mPdWpi6xxMbyST.\n",
      "Training Response: {\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"id\": \"ft-tj3gzvmjl4mPdWpi6xxMbyST\",\n",
      "  \"hyperparams\": {\n",
      "    \"n_epochs\": 15,\n",
      "    \"batch_size\": 3,\n",
      "    \"prompt_loss_weight\": 0.01,\n",
      "    \"learning_rate_multiplier\": 0.3\n",
      "  },\n",
      "  \"organization_id\": \"org-6eT84cKNSkcX5WqEyFAc5rPD\",\n",
      "  \"model\": \"davinci\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"object\": \"file\",\n",
      "      \"id\": \"file-2sCcE5tGNelmNGrMElshJlMw\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"filename\": \"file\",\n",
      "      \"bytes\": 1356,\n",
      "      \"created_at\": 1696240295,\n",
      "      \"status\": \"processed\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"validation_files\": [\n",
      "    {\n",
      "      \"object\": \"file\",\n",
      "      \"id\": \"file-YdohajIDRBOTOZn0wRphiJ8z\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"filename\": \"file\",\n",
      "      \"bytes\": 1044,\n",
      "      \"created_at\": 1696240296,\n",
      "      \"status\": \"processed\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"result_files\": [],\n",
      "  \"created_at\": 1696240539,\n",
      "  \"updated_at\": 1696240539,\n",
      "  \"status\": \"pending\",\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"object\": \"fine-tune-event\",\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-tj3gzvmjl4mPdWpi6xxMbyST\",\n",
      "      \"created_at\": 1696240539\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Training Status: pending\n"
     ]
    }
   ],
   "source": [
    "# # The code above generates the following information for the jobID: \n",
    "# (ft-CfuVdcqEYfPcbLPbbnVnd2kh), the training response, and the training status (pending).\n",
    "\n",
    "\n",
    "create_args = {\n",
    "\t\"training_file\": \"file-2sCcE5tGNelmNGrMElshJlMw\",\n",
    "\t\"validation_file\": \"file-YdohajIDRBOTOZn0wRphiJ8z\",\n",
    "\t\"model\": \"davinci\",\n",
    "\t\"n_epochs\": 15,\n",
    "\t\"batch_size\": 3,\n",
    "\t\"learning_rate_multiplier\": 0.3\n",
    "}\n",
    "\n",
    "response = openai.FineTune.create(**create_args)\n",
    "job_id = response[\"id\"]\n",
    "status = response[\"status\"]\n",
    "\n",
    "print(f'Fine-tunning model with jobID: {job_id}.')\n",
    "print(f\"Training Response: {response}\")\n",
    "print(f\"Training Status: {status}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming events for the fine-tuning job: ft-tj3gzvmjl4mPdWpi6xxMbyST\n",
      "2023-10-02 11:55:39 Created fine-tune: ft-tj3gzvmjl4mPdWpi6xxMbyST\n",
      "2023-10-02 11:57:43 Fine-tune costs $0.11\n",
      "2023-10-02 11:57:43 Fine-tune enqueued. Queue number: 0\n",
      "2023-10-02 11:57:45 Fine-tune started\n"
     ]
    }
   ],
   "source": [
    "# This pending status does not provide any relevant information.\n",
    "# However, we can have more insight into the training process by running the following code:\n",
    "\n",
    "import signal\n",
    "import datetime\n",
    "\n",
    "def signal_handler(sig, frame):\n",
    "    status = openai.FineTune.retrieve(job_id).status\n",
    "    print(f\"Stream interrupted. Job is still {status}.\")\n",
    "    return\n",
    "\n",
    "print(f'Streaming events for the fine-tuning job: {job_id}')\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "events = openai.FineTune.stream_events(job_id)\n",
    "try:\n",
    "    for event in events:\n",
    "        print(f'{datetime.datetime.fromtimestamp(event[\"created_at\"])} {event[\"message\"]}')\n",
    "\n",
    "except Exception:\n",
    "    print(\"Stream interrupted (client disconnected).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune job ft-tj3gzvmjl4mPdWpi6xxMbyST finished with status: succeeded\n",
      "Checking other finetune jobs in the subscription.\n",
      "Found 4 finetune jobs.\n"
     ]
    }
   ],
   "source": [
    "# Check the fine-tuning job status\n",
    "# Let's verify that our operation was successful, and additionally, \n",
    "# we can examine all the fine-tuning operations by using a list operation.\n",
    "\n",
    "import time\n",
    "\n",
    "status = openai.FineTune.retrieve(id=job_id)[\"status\"]\n",
    "if status not in [\"succeeded\", \"failed\"]:\n",
    "    print(f'Job not in terminal status: {status}. Waiting.')\n",
    "    while status not in [\"succeeded\", \"failed\"]:\n",
    "        time.sleep(2)\n",
    "        status = openai.FineTune.retrieve(id=job_id)[\"status\"]\n",
    "        print(f'Status: {status}')\n",
    "else:\n",
    "    print(f'Finetune job {job_id} finished with status: {status}')\n",
    "\n",
    "print('Checking other finetune jobs in the subscription.')\n",
    "result = openai.FineTune.list()\n",
    "print(f'Found {len(result.data)} finetune jobs.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "davinci:ft-hal149:superhero-2023-09-25-10-24-32\n"
     ]
    }
   ],
   "source": [
    "# Validation of the model\n",
    "# Finally, the fine-tuned model can be retrieved from the “fine_tuned_model” attribute.\n",
    "# The following print statement shows what the name of the final mode is.\n",
    "\n",
    "\n",
    "# Retrieve the fine-tuned model from the result\n",
    "fine_tuned_model = result[\"data\"][0][\"fine_tuned_model\"]\n",
    "\n",
    "# Print the fine-tuned model\n",
    "print(fine_tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It is a small, round bone found in an adult's ear, one that\n",
      "\n",
      "\n",
      "A.\n",
      "\n",
      "Oxygen\n",
      "\n",
      "B.\n",
      "\n",
      "Hyd\n"
     ]
    }
   ],
   "source": [
    "# With this model, we can run queries to validate its results by providing:\n",
    "# a prompt, the model name, and creating a query with the openai.Completion.create() function. \n",
    "# The result is retrieved from the answer dictionary as follows:\n",
    "\n",
    "new_prompt = \"Which part is the smallest bone in the entire human body?\"\n",
    "answer = openai.Completion.create(\n",
    "  model=fine_tuned_model,\n",
    "  prompt=new_prompt\n",
    ")\n",
    "\n",
    "print(answer['choices'][0]['text'])\n",
    "\n",
    "new_prompt = \"\"\" Which type of gas is utilized by plants during the process of photosynthesis?\"\"\"\n",
    "answer = openai.Completion.create(\n",
    "  model=fine_tuned_model,\n",
    "  prompt=new_prompt\n",
    ")\n",
    "\n",
    "print(answer['choices'][0]['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
