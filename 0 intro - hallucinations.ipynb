{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing hallucinations \n",
    "\n",
    "!!! Actualizar modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai\n",
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar métodos, API y crear una variable ambiente con el modelo que vamos a usar\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from transformers import GPT2TokenizerFast\n",
    "from typing import Dict, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear variable ambiente con el modelo y API \n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "COMPLETIONS_MODEL = \"gpt-3.5-turbo-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora hacemos una consulta sobre la que el modelo no haya sido entrenado para que el modelo se invente la respuesta. \n",
    "# Marcelo Chierghini es un olimpista brasileño, pero es nadador y quedó en el puesto 8 de las olimpiadas.\n",
    "\n",
    "prompt = \"Who won the 2020 Summer Olympics men's high jump?\"\n",
    "# prompt = \"Quién ganó las olimpiadas de verano 2020 en la categoría de salto de altura de hombres?\"\n",
    "\n",
    "openai.createCompletion(\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=300,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    model=COMPLETIONS_MODEL\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prevenir alucinaciones\n",
    "\n",
    "Vamos a tratar una serie de métodos para prevenir las alucinaciones por este orden: \n",
    "- Hacer reconocer al modelo que no sabe la respuesta (en este notebook)\n",
    "- Mejorar la consulta, es decir el prompt (en este notebook)\n",
    "- Utilizar embeddings.\n",
    "- Fine-tunear el modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer que el modelo responda que no sabe a respuesta: \n",
    "\n",
    "prompt = \"\"\"Responde a la pregunta lo más verídicamente posible, y si no estás seguro de la respuesta responde 'Lo siento, no lo sé'. \n",
    "\n",
    "Q: Quién ganó las olimpiadas de verano 2020 en la categoría de salto de altura de hombres? \n",
    "A:\"\"\"\n",
    "\n",
    "openai.Completion.create(\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=300,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    model=COMPLETIONS_MODEL\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejorar el prompt dando pistas sobre la respuesta, además de lo anterior. \n",
    "# Observar cómo si la cantidad de contexto no es muy grande, podemos incluirla en el prompt directamente. \n",
    "\n",
    "prompt = \"\"\"Answer the question as truthfully as possible using the provided text, and if the answer is not contained within the text below, say \"I don't know\"\n",
    "\n",
    "Context:\n",
    "The men's high jump event at the 2020 Summer Olympics took place between 30 July and 1 August 2021 at the Olympic Stadium.\n",
    "33 athletes from 24 nations competed; the total possible number depended on how many nations would use universality places \n",
    "to enter athletes in addition to the 32 qualifying through mark or ranking (no universality places were used in 2021).\n",
    "Italian athlete Gianmarco Tamberi along with Qatari athlete Mutaz Essa Barshim emerged as joint winners of the event following\n",
    "a tie between both of them as they cleared 2.37m. Both Tamberi and Barshim agreed to share the gold medal in a rare instance\n",
    "where the athletes of different nations had agreed to share the same medal in the history of Olympics. \n",
    "Barshim in particular was heard to ask a competition official \"Can we have two golds?\" in response to being offered a \n",
    "'jump off'. Maksim Nedasekau of Belarus took bronze. The medals were the first ever in the men's high jump for Italy and \n",
    "Belarus, the first gold in the men's high jump for Italy and Qatar, and the third consecutive medal in the men's high jump\n",
    "for Qatar (all by Barshim). Barshim became only the second man to earn three medals in high jump, joining Patrik Sjöberg\n",
    "of Sweden (1984 to 1992).\n",
    "\n",
    "Q: Who won the 2020 Summer Olympics men's high jump?\n",
    "A:\"\"\"\n",
    "\n",
    "openai.Completion.create(\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=300,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    model=COMPLETIONS_MODEL\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating hallucinations with numeric values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Find the result of 3 plus 6.'\n",
      "Correct Answer: '9'\n",
      "Correct Count: 19 out of 125 responses\n",
      "Accuracy: 15.20%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import openai\n",
    "import collections\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Define the prompts and correct answers\n",
    "prompts = [\n",
    "    \"Find the result of 3 plus 6.\",\n",
    "    \"Find the result of 3 plus 6.\",\n",
    "    \"Find the result of 3 plus 6.\",\n",
    "    \"Find the result of 3 plus 6.\",\n",
    "    \"Find the result of 3 plus 6.\"\n",
    "]\n",
    "correct_answer = \"9\"\n",
    "\n",
    "# Number of queries for each prompt\n",
    "num_queries = 25\n",
    "\n",
    "# Initialize responses dictionary\n",
    "responses = {prompt: [] for prompt in prompts}\n",
    "\n",
    "# Send queries and accumulate responses\n",
    "for prompt in prompts:\n",
    "    for _ in range(num_queries):\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"text-davinci-003\",\n",
    "            prompt=prompt,\n",
    "            max_tokens=50,\n",
    "        )\n",
    "        response_text = response.choices[0].text.strip()\n",
    "        responses[prompt].append(response_text)\n",
    "\n",
    "# Calculate statistics for each prompt\n",
    "for prompt, answers in responses.items():\n",
    "    answer_distribution = collections.Counter(answers)\n",
    "    correct_count = answer_distribution[correct_answer]\n",
    "    total_responses = len(answers)\n",
    "    \n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"Correct Answer: '{correct_answer}'\")\n",
    "    print(f\"Correct Count: {correct_count} out of {total_responses} responses\")\n",
    "    print(f\"Accuracy: {correct_count / total_responses:.2%}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating halluciations statistics and with simbolic reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: '2 + 5 + 2 = 9' - Count: 19\n",
      "Answer: '14' - Count: 2\n",
      "Answer: '20' - Count: 1\n",
      "Answer: '4 + 10 + 10 = 24' - Count: 1\n",
      "Answer: 'Answer: 24' - Count: 1\n",
      "Answer: '22' - Count: 1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import openai\n",
    "import collections\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# prompt\n",
    "prompt = \"what is the value of 2 * ( 5 * 2 ) replacing '*' by the addition operator?\"\n",
    "\n",
    "# iteraciones\n",
    "num_queries = 25\n",
    "\n",
    "# inicializar respuestas\n",
    "responses = []\n",
    "\n",
    "# enviar preguntas y acumular respuestas: \n",
    "for _ in range(num_queries):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",  \n",
    "        prompt=prompt,\n",
    "        max_tokens=50,    \n",
    "    )\n",
    "\n",
    "    responses.append(response.choices[0].text.strip())\n",
    "\n",
    "# tabla de respuestas: \n",
    "answer_distribution = collections.Counter(responses)\n",
    "\n",
    "# imprimir la distribución\n",
    "for answer, count in answer_distribution.most_common():\n",
    "    print(f\"Answer: '{answer}' - Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: gpt-4\n",
      "Response: The numeric value of 2 * ( 5 * 2 ), replacing '*' by the exponential operator,\n",
      "would be 2^ (5^2) = 2^25 = 33,554,432.\n",
      "\n",
      "Model: gpt-3.5-turbo\n",
      "Response: The exponentiation operator is denoted by the symbol '^' in many programming\n",
      "languages. Using this operator, the expression \"2 * ( 5 * 2 )\" would be\n",
      "calculated as follows: 2 * (5 * 2) = 2 * 10 = 20 Therefore, the numeric value of\n",
      "the expression is 20.\n",
      "\n",
      "Model: gpt-3.5-turbo-0301\n",
      "Response: Assuming that by \"exponential operator\" you mean the multiplication operator\n",
      "denoted by \"*\", the numeric value of 2 * (5 * 2) is 20. The expression 5 * 2\n",
      "inside the parentheses is evaluated first and results in 10. Then, 2 is\n",
      "multiplied by 10 to obtain the final result of 20. There is no exponentiation\n",
      "involved in this calculation.\n",
      "\n",
      "Model: gpt-3.5-turbo-0613\n",
      "Response: To replace the '*' symbol with the exponential operator, you would rewrite the\n",
      "expression as follows: 2 * (5 * 2) becomes 2 * (5^2). The exponentiation\n",
      "operator (^) is used to raise a number to a certain power. In this case, 5^2\n",
      "means raising 5 to the power of 2, which is 25. So, the numeric value of the\n",
      "expression 2 * (5 * 2) with the '*' replaced by the exponential operator would\n",
      "be 2 * 25, which is equal to 50.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating hallucinations for different models\n",
    "\n",
    "\n",
    "import os\n",
    "import openai\n",
    "import textwrap\n",
    "\n",
    "# Get the API key\n",
    "api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Set up the OpenAI API key\n",
    "openai.api_key = api_key\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"what is the numeric value of 2 * ( 5 * 2 ) replacing '*' by the exponential operator? \"\n",
    "\n",
    "# Define the list of models\n",
    "models = [\"gpt-4\",\"gpt-3.5-turbo\", \"gpt-3.5-turbo-0301\", \"gpt-3.5-turbo-0613\"]\n",
    "\n",
    "# Maximum number of words for each response\n",
    "max_words = 100\n",
    "\n",
    "# Generate a response for each model\n",
    "for model in models:\n",
    "    try:\n",
    "        # Generate a response with the API\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model, messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        # Extract the response text from the API response\n",
    "        generated_response = response['choices'][0]['message']['content']\n",
    "\n",
    "        # Split the response into words\n",
    "        words = generated_response.split()\n",
    "\n",
    "        # Truncate the response to the maximum number of words\n",
    "        truncated_response = ' '.join(words[:max_words])\n",
    "\n",
    "        # Print the response with text wrapping\n",
    "        wrapped_response = textwrap.fill(truncated_response, width=80)  # Adjust the width as needed\n",
    "        print(f\"Model: {model}\\nResponse: {wrapped_response}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error for model {model}: {e}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
